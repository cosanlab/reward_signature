{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824dccff",
   "metadata": {},
   "source": [
    "# Estimate First-Level Models\n",
    "\n",
    "*Written by Luke Chang\n",
    "\n",
    "This notebook contains code used to estimate first level models for a number of the datasets.\n",
    "\n",
    "- HCP\n",
    "    - Includes code to download from HCP AWS S3 Bucket\n",
    "    - Gambling\n",
    "    - Social\n",
    "    - WM\n",
    "    - Language\n",
    "- MIT MID \n",
    "    - Collected by Co-Author Livia Tomova\n",
    "    - Accessed via https://openneuro.org/datasets/ds003242/versions/1.0.0\n",
    "    - Includes GLM code\n",
    "- Vanderbilt MID\n",
    "    - Shared by Co-Authors Greg Samanez-Larkin, Jaime Castrellon, & David Zald\n",
    "- Rutgers Social Gambling\n",
    "    - Shared by Co-Authors Dominic Fareri & Maurcio Delgado\n",
    "- Rutgers Trust Game\n",
    "    - Shared by Co-Authors Dominic Fareri & Maurcio Delgado\n",
    "- Neurosynth Gene Expression\n",
    "    - Shared by Co-Author Luke Chang\n",
    "    - Available on https://neurosynth.org/)\n",
    "- Vanderbilt PET\n",
    "    - Shared by Co-Authors Greg Samanez-Larkin, Jaime Castrellon, & David Zald\n",
    "- Sweet Taste\n",
    "    - Accessed via https://openneuro.org/datasets/ds000229/versions/00001\n",
    "    - Includes GLM Code\n",
    "- Milkshake Data \n",
    "    - Shared by Eric Stice and Sonja Yokum\n",
    "- UCLA BART\n",
    "    - Accessed via https://openneuro.org/datasets/ds000030/versions/1.0.0\n",
    "    - Includes GLM Code\n",
    "- Self Referential\n",
    "    - Shared by Co-Author Andy Chen\n",
    "- Boulder Pain\n",
    "    - Shared by Co-Author Luke Chang\n",
    "    - Available at https://neurovault.org/collections/504/\n",
    "- Pittsburgh IAPS\n",
    "    - Shared by Co-Author Luke Chang\n",
    "    - Available at https://neurovault.org/collections/1964/\n",
    "- Stop Signal Task\n",
    "    - Accessed via https://neurovault.org/collections/1807/\n",
    "- MIT Deprivation\n",
    "    - Collected by Co-Author Livia Tomova\n",
    "    - Accessed via https://openneuro.org/datasets/ds003242/versions/1.0.0\n",
    "    - Includes GLM Code\n",
    "- Mixed Gamble Task\n",
    "    - Accessed via https://openneuro.org/datasets/ds000005/versions/00001\n",
    "    - Includes GLM Code\n",
    "- Naturalistic Viewing\n",
    "    - Shared by Co-Author Luke Chang\n",
    "    - Available at https://openneuro.org/datasets/ds003521/versions/1.0.0\n",
    "- Facial Expressions\n",
    "    - Shared by Co-Author Luke Chang\n",
    "    - Available at https://osf.io/f9gyd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from nltools.data import Brain_Data, Design_Matrix\n",
    "from nltools.mask import expand_mask, roi_to_brain\n",
    "from nltools.stats import zscore, regress, find_spikes\n",
    "from nltools.file_reader import onsets_to_dm\n",
    "from nltools.external import glover_hrf\n",
    "from nilearn.plotting import view_img, glass_brain, plot_stat_map\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def make_motion_covariates(mc, tr=2.0):\n",
    "    '''Create motion covariates regressors from realignment parameters\n",
    "    \n",
    "    Args:\n",
    "        mc: (pd.DataFrame) realignment parameters\n",
    "        tr: (float) repetition time\n",
    "        \n",
    "    Returns:\n",
    "        design_matrix: (Design_Matrix) instance\n",
    "\n",
    "    '''\n",
    "    \n",
    "    z_mc = zscore(mc)\n",
    "    all_mc = pd.concat([z_mc, z_mc**2, z_mc.diff(), z_mc.diff()**2], axis=1)\n",
    "    all_mc.fillna(value=0, inplace=True)\n",
    "    return Design_Matrix(all_mc, sampling_freq=1/tr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bdd78",
   "metadata": {},
   "source": [
    "# Download and Clean HCP Data \n",
    "Data is downloaded from OpenAccess AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Storage/Data/HCP'\n",
    "\n",
    "meta = pd.read_csv(os.path.join(base_dir, 'unrestricted_ljchang_5_16_2021_18_44_14.csv'))\n",
    "sub_list = list(meta['Subject'].values)\n",
    "\n",
    "tasks = ['GAMBLING','LANGUAGE','SOCIAL','WM']\n",
    "\n",
    "for sub in tqdm(sub_list):\n",
    "    for task in tasks:\n",
    "        if not os.path.exists(os.path.join(base_dir, 'Data', str(sub))):\n",
    "            os.mkdir(os.path.join(base_dir, 'Data', str(sub)))\n",
    "        if not os.path.exists(os.path.join(base_dir, 'Data', str(sub), task)):\n",
    "            os.mkdir(os.path.join(base_dir, 'Data', str(sub), task))\n",
    "        if task == 'WM':\n",
    "            !aws s3 cp s3://hcp-openaccess/HCP/{sub}/MNINonLinear/Results/tfMRI_{task}/tfMRI_{task}_hp200_s4_level2vol.feat/cope9.feat/stats/pe1.nii.gz /Storage/Data/HCP/Data/{sub}/{task}/cope9/\n",
    "            !aws s3 cp s3://hcp-openaccess/HCP/{sub}/MNINonLinear/Results/tfMRI_{task}/tfMRI_{task}_hp200_s4_level2vol.feat/cope10.feat/stats/pe1.nii.gz /Storage/Data/HCP/Data/{sub}/{task}/cope10/      \n",
    "        else:\n",
    "            !aws s3 cp s3://hcp-openaccess/HCP/{sub}/MNINonLinear/Results/tfMRI_{task}/tfMRI_{task}_hp200_s4_level2vol.feat/cope1.feat/stats/pe1.nii.gz /Storage/Data/HCP/Data/{sub}/{task}/cope1/\n",
    "            !aws s3 cp s3://hcp-openaccess/HCP/{sub}/MNINonLinear/Results/tfMRI_{task}/tfMRI_{task}_hp200_s4_level2vol.feat/cope2.feat/stats/pe1.nii.gz /Storage/Data/HCP/Data/{sub}/{task}/cope2/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d8317",
   "metadata": {},
   "source": [
    "## Clean Gambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'GAMBLING'\n",
    "\n",
    "sub_list = list(set([x.split('/')[5] for x in file_list_1]) & set([x.split('/')[5] for x in file_list_2]))\n",
    "sub_list.sort()\n",
    "file_list_1 = []; file_list_2 = [];\n",
    "for sub in sub_list:\n",
    "    file_list_1.append(os.path.join(base_dir, 'Data', sub, task, 'cope1','pe1.nii.gz'))\n",
    "    file_list_2.append(os.path.join(base_dir, 'Data', sub, task, 'cope2','pe1.nii.gz'))\n",
    "    \n",
    "print(len(file_list_1), len(file_list_2))\n",
    "punish = Brain_Data(file_list_1)\n",
    "reward = Brain_Data(file_list_2)\n",
    "\n",
    "# Write out as hdf5\n",
    "punish.write(os.path.join(base_dir, 'Analyses',task, f'HCP_{task}_Punish_n{len(punish)}.hdf5'))\n",
    "reward.write(os.path.join(base_dir, 'Analyses',task, f'HCP_{task}_Reward_n{len(punish)}.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c438da3",
   "metadata": {},
   "source": [
    "## Clean Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a018a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'SOCIAL'\n",
    "\n",
    "sub_list = list(set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope1','pe1.nii.gz'))]) & set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope2','pe1.nii.gz'))]))\n",
    "sub_list.sort()\n",
    "file_list_1 = []; file_list_2 = [];\n",
    "for sub in sub_list:\n",
    "    file_list_1.append(os.path.join(base_dir, 'Data', sub, task, 'cope1','pe1.nii.gz'))\n",
    "    file_list_2.append(os.path.join(base_dir, 'Data', sub, task, 'cope2','pe1.nii.gz'))\n",
    "\n",
    "print(len(file_list_1), len(file_list_2))\n",
    "random = Brain_Data(file_list_1)\n",
    "tom = Brain_Data(file_list_2)\n",
    "\n",
    "# Write out as hdf5\n",
    "random.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_Random_n{len(random)}.hdf5'))\n",
    "tom.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_TOM_n{len(tom)}.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0c164",
   "metadata": {},
   "source": [
    "## Clean Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'LANGUAGE'\n",
    "\n",
    "sub_list = list(set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope1','pe1.nii.gz'))]) & set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope2','pe1.nii.gz'))]))\n",
    "sub_list.sort()\n",
    "file_list_1 = []; file_list_2 = [];\n",
    "for sub in sub_list:\n",
    "    file_list_1.append(os.path.join(base_dir, 'Data', sub, task, 'cope1','pe1.nii.gz'))\n",
    "    file_list_2.append(os.path.join(base_dir, 'Data', sub, task, 'cope2','pe1.nii.gz'))\n",
    "\n",
    "print(len(file_list_1), len(file_list_2))\n",
    "math = Brain_Data(file_list_1)\n",
    "story = Brain_Data(file_list_2)\n",
    "\n",
    "# Write out as hdf5\n",
    "math.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_Math_n{len(random)}.hdf5'))\n",
    "story.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_Story_n{len(tom)}.hdf5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584af56a",
   "metadata": {},
   "source": [
    "## Clean Working Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be027fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'WM'\n",
    "\n",
    "sub_list = list(set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope9','pe1.nii.gz'))]) & set([x.split('/')[5] for x in glob.glob(os.path.join(base_dir, 'Data','*', task, 'cope10','pe1.nii.gz'))]))\n",
    "sub_list.sort()\n",
    "file_list_1 = []; file_list_2 = [];\n",
    "for sub in sub_list:\n",
    "    file_list_1.append(os.path.join(base_dir, 'Data', sub, task, 'cope9','pe1.nii.gz'))\n",
    "    file_list_2.append(os.path.join(base_dir, 'Data', sub, task, 'cope10','pe1.nii.gz'))\n",
    "\n",
    "print(len(file_list_1), len(file_list_2))\n",
    "wm_2bk = Brain_Data(file_list_1)\n",
    "wm_0bk = Brain_Data(file_list_2)\n",
    "\n",
    "# Write out as hdf5\n",
    "wm_2bk.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_2Back_n{len(random)}.hdf5'))\n",
    "wm_0bk.write(os.path.join(base_dir, 'Analyses', f'HCP_{task}', f'HCP_{task}_0Back_n{len(tom)}.hdf5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de63682",
   "metadata": {},
   "source": [
    "# MIT MID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418046a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bids_events(subject='01', session='b', run=1, tr=2.0, task='midloc'):\n",
    "    '''Create a design_matrix instance from BIDS event file\n",
    "    \n",
    "    Args:\n",
    "        subject: (str) subject id\n",
    "        session: (str) scanning session\n",
    "        run: (int) run number\n",
    "        tr: (float) repetition time\n",
    "\n",
    "    Returns:\n",
    "        design_matrix: (Design_Matrix) instance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    n_tr = nib.load(os.path.join(beta_dir, 'derivatives','fmriprep',f'sub-SAXSISO{subject}{session}','func',f'sub-SAXSISO{subject}{session}_task-{task}_run-00{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')).shape[-1]\n",
    "    events = pd.read_csv(os.path.join(beta_dir, f'sub-SAXSISO{subject}{session}','func',f'sub-SAXSISO{subject}{session}_task-{task}_run-00{run}_events.tsv'), sep='\\t')\n",
    "    events = events[['onset','duration','trial_type']]\n",
    "    events.columns = ['Onset', 'Duration', 'Stim']\n",
    "    events = events.loc[~events['Stim'].isnull()]\n",
    "    return onsets_to_dm(events, sampling_freq=1/tr, run_length=n_tr)\n",
    "\n",
    "beta_dir = '/Storage/Data/social_deprivation/'\n",
    "\n",
    "\n",
    "subject = '01'\n",
    "session = 'b'\n",
    "run = 1\n",
    "tr = 2.0\n",
    "\n",
    "# for subject in tqdm([1,2,3,4,8,9,10,11,12,13,14,15,17,18,19,21,22,24,26,27,28,30,32,33,34,35,36,38,39,40,41,42]):\n",
    "for subject in tqdm([1,2,3,4,8,10,11,12,13,14,15,17,18,19,21,24,26,27,28,30,32,33,34,35,36,38,39,41,42]):\n",
    "\n",
    "    # Load data\n",
    "    data = Brain_Data(os.path.join(beta_dir, 'derivatives','fmriprep',f'sub-SAXSISO{subject:02}{session}','func',f'sub-SAXSISO{subject:02}{session}_task-midloc_run-00{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'))\n",
    "\n",
    "    # Smooth \n",
    "    data = data.smooth(fwhm=6)\n",
    "\n",
    "    # Rescale Data so that each voxel is scaled proportional to percent signal change\n",
    "    data = data.scale()\n",
    "    \n",
    "    # confound covars\n",
    "    confounds = pd.read_csv(os.path.join(beta_dir, 'derivatives','fmriprep',f'sub-SAXSISO{subject:02}{session}','func',f'sub-SAXSISO{subject:02}{session}_task-midloc_run-00{run}_desc-confounds_regressors.tsv'), sep='\\t')\n",
    "\n",
    "    # motion covars\n",
    "    mc = confounds[['trans_x','trans_y','trans_z','rot_x', 'rot_y', 'rot_z']]\n",
    "    mc_cov = make_motion_covariates(mc, tr)\n",
    "\n",
    "    # events\n",
    "    onsets = load_bids_events(subject=f'{subject:02}', session=session, run=run, tr=tr)\n",
    "\n",
    "    # spikes\n",
    "    spikes = data.find_spikes(global_spike_cutoff=3, diff_spike_cutoff=3)\n",
    "\n",
    "    # Convolve, add linear drift, dct, intercept\n",
    "    dm_cov = onsets.convolve().add_dct_basis(duration=120).add_poly(order=1, include_lower=True)\n",
    "\n",
    "    # add spikes\n",
    "    dm_cov = dm_cov.append(mc_cov, axis=1).append(Design_Matrix(spikes.iloc[:, 1:], sampling_freq=1/tr), axis=1)\n",
    "\n",
    "    #regress\n",
    "    data.X = dm_cov\n",
    "    stats = data.regress()\n",
    "\n",
    "    write out betas\n",
    "    reward = stats['beta'][np.where(data.X.columns == 'Rewarded_c0')[0]]\n",
    "    reward.write(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', f'sub-SAXSISO{subject:02}b_task-midloc_Reward_beta_smooth6mm.nii.gz'))\n",
    "\n",
    "    noreward = stats['beta'][np.where(data.X.columns == 'NonRewarded_c0')[0]]\n",
    "    noreward.write(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', f'sub-SAXSISO{subject:02}b_task-midloc_NoReward_beta_smooth6mm.nii.gz'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7222d87",
   "metadata": {},
   "source": [
    "# MIT Social Deprivation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7dab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tr = 2.0\n",
    "task = 'ls'\n",
    "\n",
    "for subject in tqdm([1,2,3,4,8,9,10,11,12,13,14,15,17,18,19,21,22,24,26,27,28,30,32,33,34,35,36,38,39,40,41,42]):\n",
    "    for session in ['b','f','s']:\n",
    "        for run in range(1,7):\n",
    "            try:\n",
    "                # Load data\n",
    "                data = Brain_Data(os.path.join(beta_dir, 'derivatives','fmriprep',f'sub-SAXSISO{subject:02}{session}','func',f'sub-SAXSISO{subject:02}{session}_task-{task}_run-00{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'))\n",
    "\n",
    "                # Smooth \n",
    "                data = data.smooth(fwhm=6)\n",
    "\n",
    "                # Rescale Data so that each voxel is scaled proportional to percent signal change\n",
    "                data = data.scale()\n",
    "\n",
    "                # confound covars\n",
    "                confounds = pd.read_csv(os.path.join(beta_dir, 'derivatives','fmriprep',f'sub-SAXSISO{subject:02}{session}','func',f'sub-SAXSISO{subject:02}{session}_task-{task}_run-00{run}_desc-confounds_regressors.tsv'), sep='\\t')\n",
    "\n",
    "                # motion covars\n",
    "                mc = confounds[['trans_x','trans_y','trans_z','rot_x', 'rot_y', 'rot_z']]\n",
    "                mc_cov = make_motion_covariates(mc, tr)\n",
    "\n",
    "                # events - NOTE: load bids events function does not work for other datasets not TOMOVA\n",
    "                onsets = load_bids_events(subject=f'{subject:02}', session=session, run=run, tr=tr, task=task)\n",
    "\n",
    "                # spikes\n",
    "                spikes = data.find_spikes(global_spike_cutoff=3, diff_spike_cutoff=3)\n",
    "\n",
    "                # Convolve, add linear drift, dct, intercept\n",
    "                dm_cov = onsets.convolve().add_dct_basis(duration=120).add_poly(order=1, include_lower=True)\n",
    "\n",
    "                # add spikes\n",
    "                dm_cov = dm_cov.append(mc_cov, axis=1).append(Design_Matrix(spikes.iloc[:, 1:], sampling_freq=1/tr), axis=1)\n",
    "\n",
    "                #regress\n",
    "                data.X = dm_cov\n",
    "                stats = data.regress()\n",
    "\n",
    "                # Create Condition Average Contrasts\n",
    "                food_contrast = (stats['beta'][np.array([x.split('_')[0] for x in data.X.columns]) == 'Food']).mean()\n",
    "                social_contrast = (stats['beta'][np.array([x.split('_')[0] for x in data.X.columns]) == 'Social']).mean()\n",
    "                control_contrast = (stats['beta'][np.array([x.split('_')[0] for x in data.X.columns]) == 'Control']).mean()\n",
    "\n",
    "                food_contrast.write(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', f'sub-SAXSISO{subject:02}{session}_task-{task}_run-00{run}_Food_beta_smooth6mm.nii.gz'))\n",
    "                social_contrast.write(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', f'sub-SAXSISO{subject:02}{session}_task-{task}_run-00{run}_Social_beta_smooth6mm.nii.gz'))\n",
    "                control_contrast.write(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', f'sub-SAXSISO{subject:02}{session}_task-{task}_run-00{run}_Control_beta_smooth6mm.nii.gz'))\n",
    "            except:\n",
    "                print(f'{subject}_{session}_{run} Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ba423",
   "metadata": {},
   "source": [
    "## Aggregate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average data across runs within subject and drop subjects without full data across sessions\n",
    "condition = ['Social','Food','Baseline']\n",
    "\n",
    "cic_sub_list = list(set([os.path.basename(x).split('_')[0] for x in glob.glob(os.path.join(beta_dir, 'derivatives','L1_Betas_Rescaled', '*task-CIC*.nii.gz'))]))\n",
    "cic_sub_list.sort()\n",
    "\n",
    "sub_list = list(set([x[:-1] for x in cic_sub_list]))\n",
    "sub_list.sort()\n",
    "\n",
    "all_session_data = {}\n",
    "missing = []\n",
    "for session in ['s', 'b', 'f']:\n",
    "    avg_social_beta = {}\n",
    "    avg_baseline_beta = {}\n",
    "    avg_food_beta = {}\n",
    "    for sub in tqdm(sub_list):\n",
    "        try:\n",
    "            avg_social_beta[sub] = Brain_Data(glob.glob(os.path.join(beta_dir, 'derivatives', 'L1_Betas_Rescaled', f'{sub}{session}_task-CIC_run-*_Social_beta_smooth6mm.nii.gz'))).mean()\n",
    "        except:\n",
    "            avg_social_beta[sub] = np.nan\n",
    "            f'{sub} - Social Not Available'\n",
    "            missing.append(sub)\n",
    "        try:\n",
    "            avg_baseline_beta[sub] = Brain_Data(glob.glob(os.path.join(beta_dir, 'derivatives', 'L1_Betas_Rescaled', f'{sub}{session}_task-CIC_run-*_Control_beta_smooth6mm.nii.gz'))).mean()\n",
    "        except:\n",
    "            avg_baseline_beta[sub] = np.nan\n",
    "            f'{sub} - Control Not Available'\n",
    "            missing.append(sub)\n",
    "        try:\n",
    "            avg_food_beta[sub] = Brain_Data(glob.glob(os.path.join(beta_dir, 'derivatives', 'L1_Betas_Rescaled', f'{sub}{session}_task-CIC_run-*_Food_beta_smooth6mm.nii.gz'))).mean()\n",
    "        except:\n",
    "            avg_food_beta[sub] = np.nan\n",
    "            f'{sub} - Food Not Available'\n",
    "            missing.append(sub)\n",
    "        all_session_data[session] = {'Social':avg_social_beta, 'Baseline':avg_baseline_beta, 'Food':avg_food_beta}\n",
    "missing = list(set(missing))\n",
    "\n",
    "# Write out Files to HDF5\n",
    "for session in ['s', 'b', 'f']:\n",
    "    for condition in ['Social', 'Baseline', 'Food']:\n",
    "        Brain_Data([all_session_data[session][condition][sub] for sub in all_session_data[session][condition] if sub not in missing]).write(os.path.join(data_dir, 'Data', 'social_isolation', f'MIT_Deprivation_Session_{session}_Images_{condition}.hdf5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaec16",
   "metadata": {},
   "source": [
    "# Sweet Taste Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bids_events(subject=1, task='flavor', run=1, tr=2.0):\n",
    "    '''Create a design_matrix instance from BIDS event file\n",
    "    \n",
    "    Args:\n",
    "        subject: (str) subject id\n",
    "        session: (str) scanning session\n",
    "        run: (int) run number\n",
    "        tr: (float) repetition time\n",
    "\n",
    "    Returns:\n",
    "        design_matrix: (Design_Matrix) instance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    n_tr = nib.load(os.path.join(data_dir, 'derivatives','fmriprep', f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run:02}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')).shape[-1]\n",
    "    events = pd.read_csv(os.path.join(data_dir, f'sub-{subject:02}','func', f'sub-{subject:02}_task-{task}_run-{run:02}_events.tsv'), sep='\\t')\n",
    "    events = events[['onset','duration','stimulus']]\n",
    "    events.columns = ['Onset', 'Duration', 'Stim']\n",
    "#     events = events.loc[~events['Stim'] == 'rinse']\n",
    "    return onsets_to_dm(events, sampling_freq=1/tr, run_length=n_tr)\n",
    "\n",
    "\n",
    "data_dir = '/Storage/Data/sweet_taste'\n",
    "\n",
    "\n",
    "\n",
    "tr = 2.0\n",
    "task = 'flavor'\n",
    "\n",
    "for subject in tqdm([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]):\n",
    "    for run in range(1,4):\n",
    "        try:\n",
    "\n",
    "            # Load data\n",
    "            data = Brain_Data(os.path.join(data_dir, 'derivatives','fmriprep',f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run:02}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'))\n",
    "\n",
    "            # Smooth \n",
    "            data = data.smooth(fwhm=6)\n",
    "\n",
    "            # Rescale Data so that each voxel is scaled proportional to percent signal change\n",
    "            data = data.scale()\n",
    "\n",
    "            # confound covars\n",
    "            confounds = pd.read_csv(os.path.join(data_dir, 'derivatives', 'fmriprep', f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run:02}_desc-confounds_regressors.tsv'), sep='\\t')\n",
    "\n",
    "            # motion covars\n",
    "            mc = confounds[['trans_x','trans_y','trans_z','rot_x', 'rot_y', 'rot_z']]\n",
    "            mc_cov = make_motion_covariates(mc, tr)\n",
    "\n",
    "            # events - NOTE: load bids events function does not work for other datasets not TOMOVA\n",
    "            onsets = load_bids_events(subject=subject, run=run, tr=tr, task=task)\n",
    "\n",
    "            # spikes\n",
    "            spikes = data.find_spikes(global_spike_cutoff=3, diff_spike_cutoff=3)\n",
    "\n",
    "            # Convolve, add linear drift, dct, intercept\n",
    "            dm_cov = onsets.convolve().add_dct_basis(duration=120).add_poly(order=1, include_lower=True)\n",
    "\n",
    "            # add spikes\n",
    "            dm_cov = dm_cov.append(mc_cov, axis=1).append(Design_Matrix(spikes.iloc[:, 1:], sampling_freq=1/tr), axis=1)\n",
    "\n",
    "            #regress\n",
    "            data.X = dm_cov\n",
    "            stats = data.regress()\n",
    "\n",
    "            # Write out files\n",
    "            for i, condition in enumerate(list(data.X.columns[:7])):\n",
    "                stats['beta'][i].write(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-{subject:02}_task-{task}_run-{run:02}_{condition}.nii.gz'))\n",
    "        \n",
    "        except:\n",
    "            print(f'{subject}_{run} Failed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720b574",
   "metadata": {},
   "source": [
    "## Aggregate Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f971d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = list(set([os.path.basename(x).split('_')[3] for x in glob.glob(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-*_task-flavor_run-*_*.nii.gz'))]))\n",
    "\n",
    "all_data = {x:[] for x in conditions}\n",
    "for subject in tqdm(range(1,16)):\n",
    "    for condition in conditions:\n",
    "        all_data[condition].append(Brain_Data(glob.glob(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-{subject:02}_task-flavor_run-*_{condition}_c0.nii.gz'))).mean())\n",
    "\n",
    "for condition in list(all_data.keys()):\n",
    "    Brain_Data(all_data[condition]).write(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'Sweet_Taste_{condition}.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85070a",
   "metadata": {},
   "source": [
    "# UCLA BART Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bids_events(subject=1, task='bart', tr=2.0):\n",
    "    '''Create a design_matrix instance from BIDS event file\n",
    "    \n",
    "    Args:\n",
    "        subject: (str) subject id\n",
    "        session: (str) scanning session\n",
    "        run: (int) run number\n",
    "        tr: (float) repetition time\n",
    "\n",
    "    Returns:\n",
    "        design_matrix: (Design_Matrix) instance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    n_tr = nib.load(os.path.join(data_dir, 'derivatives','fmriprep', subject,'func',f'{subject}_task-{task}_bold_space-MNI152NLin2009cAsym_preproc.nii.gz')).shape[-1]\n",
    "    events = pd.read_csv(os.path.join(data_dir, subject,'func', f'{subject}_task-{task}_events.tsv'), sep='\\t')\n",
    "    events['stimulus'] = np.nan\n",
    "    for i,row in events.iterrows():\n",
    "        if row['action'] == 'EXPLODE':\n",
    "            condition = 'explode'\n",
    "        elif row['action'] == 'CASHOUT':\n",
    "            condition = 'cash'\n",
    "        elif row['action'] == 'ACCEPT':\n",
    "            if row['trial_type']=='BALOON':\n",
    "                condition = 'pumps'\n",
    "            elif row['trial_type']=='CONTROL':\n",
    "                condition = 'control_pumps'\n",
    "\n",
    "        events.loc[i,'stimulus'] = condition\n",
    "    events.loc[events['stimulus']=='explode', 'duration'] = 1\n",
    "    events = events[['onset','duration','stimulus']]\n",
    "    events.dropna(inplace=True)\n",
    "    events.columns = ['Onset', 'Duration', 'Stim']\n",
    "    return onsets_to_dm(events, sampling_freq=1/tr, run_length=n_tr)\n",
    "\n",
    "\n",
    "data_dir = '/Storage/Data/Neuropsychiatric_Phenomics_LA5c/'\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(data_dir, 'participants.tsv'), sep='\\t')\n",
    "sub_list = list(metadata.query('diagnosis==\"CONTROL\"')['participant_id'])\n",
    "\n",
    "task = 'bart'\n",
    "tr = 2.0\n",
    "fwhm = 6\n",
    "\n",
    "missing_subs = []\n",
    "for subject in tqdm(sub_list[3:]):\n",
    "    try:\n",
    "        data = Brain_Data(os.path.join(data_dir, 'derivatives','fmriprep', subject,'func',f'{subject}_task-{task}_bold_space-MNI152NLin2009cAsym_preproc.nii.gz'))\n",
    "\n",
    "        # smooth\n",
    "        data = data.smooth(fwhm)\n",
    "\n",
    "        # confound covars\n",
    "        confounds = pd.read_csv(os.path.join(data_dir, 'derivatives', 'fmriprep', subject,'func',f'{subject}_task-{task}_bold_confounds.tsv'), sep='\\t')\n",
    "\n",
    "        # motion covars\n",
    "        mc = confounds[['X','Y','Z','RotX', 'RotY', 'RotZ']]\n",
    "        mc_cov = make_motion_covariates(mc, tr)\n",
    "\n",
    "        # events - NOTE: load bids events function does not work for other datasets not TOMOVA\n",
    "        onsets = load_bids_events(subject=subject, tr=tr, task=task)\n",
    "\n",
    "        # spikes\n",
    "        spikes = data.find_spikes(global_spike_cutoff=3, diff_spike_cutoff=3)\n",
    "\n",
    "        # Convolve, add linear drift, dct, intercept\n",
    "        dm_cov = onsets.convolve().add_dct_basis(duration=120).add_poly(order=1, include_lower=True)\n",
    "\n",
    "        # add spikes\n",
    "        dm_cov = dm_cov.append(mc_cov, axis=1).append(Design_Matrix(spikes.iloc[:, 1:], sampling_freq=1/tr), axis=1)\n",
    "\n",
    "        #regress\n",
    "        data.X = dm_cov\n",
    "        stats = data.regress()\n",
    "\n",
    "        # Write out files\n",
    "        for i, condition in enumerate([x for x in data.X.columns if '_c0' in x]):\n",
    "            stats['beta'][i].write(os.path.join(data_dir, 'derivatives', 'L1_Betas', 'bart', 'smoothed', f'{subject}_task-{task}_{condition[:-3]}.nii.gz'))\n",
    "    except:\n",
    "        missing_subs.append(subject)\n",
    "        print(f'Error with {subject}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42526d3f",
   "metadata": {},
   "source": [
    "# UCLA Mixed Gambles - Loss Aversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05448358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bids_events(subject=1, task='mixedgamblestask', run=1, tr=2.0):\n",
    "    '''Create a design_matrix instance from BIDS event file\n",
    "    \n",
    "    Args:\n",
    "        subject: (str) subject id\n",
    "        session: (str) scanning session\n",
    "        run: (int) run number\n",
    "        tr: (float) repetition time\n",
    "\n",
    "    Returns:\n",
    "        design_matrix: (Design_Matrix) instance\n",
    "    \n",
    "    '''\n",
    "\n",
    "    n_tr = nib.load(os.path.join(data_dir, 'derivatives','fmriprep', f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')).shape[-1]\n",
    "    events = pd.read_csv(os.path.join(data_dir, f'sub-{subject:02}','func', f'sub-{subject:02}_task-{task}_run-{run:02}_events.tsv'), sep='\\t')\n",
    "    events['stimulus'] = np.nan\n",
    "    for i,row in events.iterrows():\n",
    "        events['stimulus'].iloc[i] = f\"Gain{int(row['gain'])}_Loss{int(row['loss'])}_Response{int(row['respnum'])}\"\n",
    "\n",
    "    events = events[['onset','duration','stimulus']]\n",
    "    events.columns = ['Onset', 'Duration', 'Stim']\n",
    "    return onsets_to_dm(events, sampling_freq=1/tr, run_length=n_tr)\n",
    "\n",
    "\n",
    "data_dir = '/Storage/Data/loss_aversion'\n",
    "\n",
    "tr = 2.0\n",
    "task = 'mixedgamblestask'\n",
    "\n",
    "for subject in tqdm(range(1,17)):\n",
    "    for run in range(1,3):\n",
    "        data = Brain_Data(os.path.join(data_dir, 'derivatives','fmriprep',f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'))\n",
    "\n",
    "        # confound covars\n",
    "        confounds = pd.read_csv(os.path.join(data_dir, 'derivatives', 'fmriprep', f'sub-{subject:02}','func',f'sub-{subject:02}_task-{task}_run-{run}_desc-confounds_timeseries.tsv'), sep='\\t')\n",
    "\n",
    "        # motion covars\n",
    "        mc = confounds[['trans_x','trans_y','trans_z','rot_x', 'rot_y', 'rot_z']]\n",
    "        mc_cov = make_motion_covariates(mc, tr)\n",
    "\n",
    "        # events - NOTE: load bids events function does not work for other datasets not TOMOVA\n",
    "        onsets = load_bids_events(subject=subject, run=run, tr=tr, task=task)\n",
    "\n",
    "        # spikes\n",
    "        spikes = data.find_spikes(global_spike_cutoff=3, diff_spike_cutoff=3)\n",
    "\n",
    "        # Convolve, add linear drift, dct, intercept\n",
    "        dm_cov = onsets.convolve().add_dct_basis(duration=120).add_poly(order=1, include_lower=True)\n",
    "\n",
    "        # add spikes\n",
    "        dm_cov = dm_cov.append(mc_cov, axis=1).append(Design_Matrix(spikes.iloc[:, 1:], sampling_freq=1/tr), axis=1)\n",
    "\n",
    "        #regress\n",
    "        data.X = dm_cov\n",
    "        stats = data.regress()\n",
    "\n",
    "        # Write out files\n",
    "        for i, condition in enumerate([x for x in data.X.columns if 'Gain' in x]):\n",
    "            stats['beta'][i].write(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-{subject:02}_task-{task}_run-{run:02}_{condition[:-3]}.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778038fd",
   "metadata": {},
   "source": [
    "## Aggregate Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72940cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = list(set([os.path.basename(x).split('_')[3] for x in glob.glob(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-*_task-flavor_run-*_*.nii.gz'))]))\n",
    "\n",
    "all_data = {x:[] for x in conditions}\n",
    "for subject in tqdm(range(1,16)):\n",
    "    for condition in conditions:\n",
    "        all_data[condition].append(Brain_Data(glob.glob(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'sub-{subject:02}_task-flavor_run-*_{condition}_c0.nii.gz'))).mean())\n",
    "\n",
    "for condition in list(all_data.keys()):\n",
    "    Brain_Data(all_data[condition]).write(os.path.join(data_dir, 'derivatives', 'L1_Betas', f'Sweet_Taste_{condition}.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb158fc6",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'demeaned_zscore1_smooth0'\n",
    "data_dir = '/Storage/Projects/Reward_Model'\n",
    "\n",
    "zscore = lambda x: (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "center = lambda x: (x - np.mean(x, axis=0))\n",
    "\n",
    "loss_aversion_metadata = pd.read_csv(os.path.join(data_dir, 'Analyses', 'Classification', f'Loss_Aversion_Similarity_{analysis_name}.csv'))\n",
    "\n",
    "loss_aversion_metadata['Gain_z'] = loss_aversion_metadata['Gain'].groupby(loss_aversion_metadata['Subject']).transform(zscore)\n",
    "loss_aversion_metadata['Loss_z'] = loss_aversion_metadata['Loss'].groupby(loss_aversion_metadata['Subject']).transform(zscore)\n",
    "\n",
    "model = Lmer(\"Similarity ~ Gain + Loss + (Gain+Loss|Subject)\", data=loss_aversion_metadata)\n",
    "model.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc720a",
   "metadata": {},
   "source": [
    "# Clean Fareri Social Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['CompPun','CompRew', 'FriendPun','FriendRew','StrangerPun','StrangerRew']\n",
    "\n",
    "sub_list = [os.path.basename(x) for x in glob.glob(os.path.join(data_dir, 'Data','Fareri_2012','Feedback_Betas','*'))]\n",
    "sub_list.sort()\n",
    "\n",
    "\n",
    "con_dat = {}\n",
    "for con in con_list:\n",
    "    con_dat[con] = []\n",
    "    for sub in tqdm(sub_list):\n",
    "        dat = Brain_Data(glob.glob(os.path.join(data_dir, 'Data','Fareri_2012','Feedback_Betas',sub, '*', f'{sub}*{con}.nii.gz')))\n",
    "        con_dat[con].append(dat[dat.std(axis=1) > 0].mean())\n",
    "    Brain_Data(con_dat[con]).write(os.path.join(data_dir, 'Data','Fareri_2012','Feedback_Summary', f'{con}.hdf5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719a51e",
   "metadata": {},
   "source": [
    "# Clean Fareri Trust Dataset\n",
    "Notes:\n",
    "- Computer conditions have 2 subjects with empty regressors, we are excluding them only in the computer condition (n=24)\n",
    "- All other conditions have 26 subjects, runs with empty regressors are excluded when calculating mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['ComputerReciprocateFB','ComputerDefectFB', 'StrangerReciprocateFB','StrangerDefectFB','FriendReciprocateFB','FriendDefectFB']\n",
    "\n",
    "sub_list = [os.path.basename(x) for x in glob.glob(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Betas','*'))]\n",
    "sub_list.sort()\n",
    "\n",
    "con_dat = {}\n",
    "for con in con_list:\n",
    "    con_dat[con] = []\n",
    "    for sub in tqdm(sub_list):\n",
    "        dat = Brain_Data(glob.glob(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Betas',sub,f'{sub}*{con}.nii.gz')))\n",
    "        con_dat[con].append(dat[dat.std(axis=1) > 0].mean())\n",
    "# Brain_Data(con_dat[con]).write(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Summary', f'{con}.hdf5'))\n",
    "\n",
    "# Check if data are empty\n",
    "con_bool = {}\n",
    "for con in con_list:\n",
    "    con_bool[con] = [isinstance(x, Brain_Data) for x in con_dat[con]]\n",
    "\n",
    "computer_sub_list = list(set(np.array(sub_list)[con_bool['ComputerReciprocateFB']]) & set(np.array(sub_list)[con_bool['ComputerDefectFB']]))\n",
    "computer_sub_list.sort()\n",
    "\n",
    "# Write out computer data\n",
    "Brain_Data([x for x in con_dat['ComputerReciprocateFB'] if isinstance(x, Brain_Data)]).write(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Summary', f'ComputerReciprocateFB.hdf5'))\n",
    "Brain_Data([x for x in con_dat['ComputerDefectFB'] if isinstance(x, Brain_Data)]).write(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Summary', f'ComputerDefectFB.hdf5'))\n",
    "\n",
    "# Write out all the other conditions\n",
    "for con in ['StrangerReciprocateFB','StrangerDefectFB','FriendReciprocateFB','FriendDefectFB']:\n",
    "    Brain_Data(con_dat[con]).write(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Summary', f'{con}.hdf5'))\n",
    "    Brain_Data(con_dat[con])[con_bool['ComputerReciprocateFB']].write(os.path.join(data_dir, 'Data','Fareri_2015','Feedback_Summary', f'{con}_n24.hdf5'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfafe2",
   "metadata": {},
   "source": [
    "# Milkshake Data\n",
    "Notes about Dataset:\n",
    "\n",
    "Stice, Burger, Yokum, 2015 J Neuro \n",
    "\n",
    "- 5 = Milkshake delivery\n",
    "- 7 = Water delivery\n",
    "\n",
    "**Contrasts**\n",
    " - con1 = milk cue - All Sessions\n",
    " - con2 = h20 cue - All Sessions'\n",
    " - con3 = milk receipt - All Sessions\n",
    " - con4 = h20 receipt - All Sessions\n",
    " - con5 = milk cue > h2o cue - All Sessions\n",
    " - con6 = h20 cue > milkshake cue - All Sessions\n",
    " - con7 = milk receipt > h20 receipt - All Sessions\n",
    " - con8 = h20 receipt > milkshake receipt - All Sessions\n",
    " - con9 = (milk cue & milk receipt ) > (h2o cue & h2o receipt) - All Sessions\n",
    " - con10 = (h2o cue & h2o receipt) > (milk cue & milk receipt ) - All Sessions\n",
    " \n",
    " \n",
    "Sonja says that beta_0005 is milkshake receipt and beta_0007 is tasteless receipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea38bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
